#!/bin/bash
# ---------------- SLURM parameters ----------------
#SBATCH -p all.q
#SBATCH --ntasks 1                 # total logical cores you requested
#SBATCH --mem=100G
#SBATCH --cpus-per-task=20          
#SBATCH -N 1
#SBATCH --mail-type=ALL
#SBATCH -J HAMNucRetSeq_pipeline
#SBATCH -D /home/pol_schiessel/maya620d/HAMNucRetSeq_pipeline
#SBATCH --output=/home/pol_schiessel/maya620d/HAMNucRetSeq_pipeline/log/Array_eukaryote.%A_%a.out
#SBATCH --error=/home/pol_schiessel/maya620d/HAMNucRetSeq_pipeline/log/Array_eukaryote.%A_%a.error


# ---------------- Load modules --------------------
module load apps/singularity


# ---------------- Runtime setup -------------------
echo "Running on $(hostname -s)   ($(lscpu | grep 'Model name' | sed 's/^.*: //'))"
echo "..."
echo "Important!!!!!! Keep this script sequential, no array job"


# LARGE_DIR="/group/pol_schiessel/Manish/HAMNucRetSeq_pipeline/output/minpoint_boundpromoter_regions_breath/crystal_freedna_md_merged"
LARGE_DIR="/group/pol_schiessel/Manish/HAMNucRetSeq_pipeline/output/exactpoint_boundpromoter_regions_breath/crystal_freedna_md_merged"
OUTPUT_DIR="${LARGE_DIR}/reduced"
mkdir -p "$OUTPUT_DIR"
# ---------------- Run the Python script -------------------
echo "Starting the filesize reduction script..."
singularity exec --bind $PWD:/project \
            hamnucret.sif python3 /project/src/utils/reduce_filesize.py \
        --input_dir   "$LARGE_DIR" \
        --output_dir  "$OUTPUT_DIR" \
        --chunksize 5000000 \
        --float_dp 14
